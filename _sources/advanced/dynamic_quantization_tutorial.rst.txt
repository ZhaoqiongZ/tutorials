
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "advanced/dynamic_quantization_tutorial.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_advanced_dynamic_quantization_tutorial.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_advanced_dynamic_quantization_tutorial.py:


(beta) Dynamic Quantization on an LSTM Word Language Model
==================================================================

**Author**: `James Reed <https://github.com/jamesr66a>`_

**Edited by**: `Seth Weidman <https://github.com/SethHWeidman/>`_

Introduction
------------

Quantization involves converting the weights and activations of your model from float
to int, which can result in smaller model size and faster inference with only a small
hit to accuracy.

In this tutorial, we will apply the easiest form of quantization -
`dynamic quantization <https://pytorch.org/docs/stable/quantization.html#torch.quantization.quantize_dynamic>`_ -
to an LSTM-based next word-prediction model, closely following the
`word language model <https://github.com/pytorch/examples/tree/master/word_language_model>`_
from the PyTorch examples.

.. GENERATED FROM PYTHON SOURCE LINES 22-32

.. code-block:: default


    # imports
    import os
    from io import open
    import time

    import torch
    import torch.nn as nn
    import torch.nn.functional as F








.. GENERATED FROM PYTHON SOURCE LINES 33-39

1. Define the model
-------------------

Here we define the LSTM model architecture, following the
`model <https://github.com/pytorch/examples/blob/master/word_language_model/model.py>`_
from the word language model example.

.. GENERATED FROM PYTHON SOURCE LINES 39-73

.. code-block:: default


    class LSTMModel(nn.Module):
        """Container module with an encoder, a recurrent module, and a decoder."""

        def __init__(self, ntoken, ninp, nhid, nlayers, dropout=0.5):
            super(LSTMModel, self).__init__()
            self.drop = nn.Dropout(dropout)
            self.encoder = nn.Embedding(ntoken, ninp)
            self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)
            self.decoder = nn.Linear(nhid, ntoken)

            self.init_weights()

            self.nhid = nhid
            self.nlayers = nlayers

        def init_weights(self):
            initrange = 0.1
            self.encoder.weight.data.uniform_(-initrange, initrange)
            self.decoder.bias.data.zero_()
            self.decoder.weight.data.uniform_(-initrange, initrange)

        def forward(self, input, hidden):
            emb = self.drop(self.encoder(input))
            output, hidden = self.rnn(emb, hidden)
            output = self.drop(output)
            decoded = self.decoder(output)
            return decoded, hidden

        def init_hidden(self, bsz):
            weight = next(self.parameters())
            return (weight.new_zeros(self.nlayers, bsz, self.nhid),
                    weight.new_zeros(self.nlayers, bsz, self.nhid))








.. GENERATED FROM PYTHON SOURCE LINES 74-82

2. Load in the text data
------------------------

Next, we load the
`Wikitext-2 dataset <https://www.google.com/search?q=wikitext+2+data>`_ into a `Corpus`,
again following the
`preprocessing <https://github.com/pytorch/examples/blob/master/word_language_model/data.py>`_
from the word language model example.

.. GENERATED FROM PYTHON SOURCE LINES 82-132

.. code-block:: default


    class Dictionary(object):
        def __init__(self):
            self.word2idx = {}
            self.idx2word = []

        def add_word(self, word):
            if word not in self.word2idx:
                self.idx2word.append(word)
                self.word2idx[word] = len(self.idx2word) - 1
            return self.word2idx[word]

        def __len__(self):
            return len(self.idx2word)


    class Corpus(object):
        def __init__(self, path):
            self.dictionary = Dictionary()
            self.train = self.tokenize(os.path.join(path, 'train.txt'))
            self.valid = self.tokenize(os.path.join(path, 'valid.txt'))
            self.test = self.tokenize(os.path.join(path, 'test.txt'))

        def tokenize(self, path):
            """Tokenizes a text file."""
            assert os.path.exists(path)
            # Add words to the dictionary
            with open(path, 'r', encoding="utf8") as f:
                for line in f:
                    words = line.split() + ['<eos>']
                    for word in words:
                        self.dictionary.add_word(word)

            # Tokenize file content
            with open(path, 'r', encoding="utf8") as f:
                idss = []
                for line in f:
                    words = line.split() + ['<eos>']
                    ids = []
                    for word in words:
                        ids.append(self.dictionary.word2idx[word])
                    idss.append(torch.tensor(ids).type(torch.int64))
                ids = torch.cat(idss)

            return ids

    model_data_filepath = 'data/'

    corpus = Corpus(model_data_filepath + 'wikitext-2')








.. GENERATED FROM PYTHON SOURCE LINES 133-149

3. Load the pretrained model
-----------------------------

This is a tutorial on dynamic quantization, a quantization technique
that is applied after a model has been trained. Therefore, we'll simply
load some pretrained weights into this model architecture; these
weights were obtained by training for five epochs using the default
settings in the word language model example.

Before running this tutorial, download the required pre-trained model:

.. code-block:: bash

    wget https://s3.amazonaws.com/pytorch-tutorial-assets/word_language_model_quantize.pth

Place the downloaded file in the data directory or update the model_data_filepath accordingly.

.. GENERATED FROM PYTHON SOURCE LINES 149-170

.. code-block:: default


    ntokens = len(corpus.dictionary)

    model = LSTMModel(
        ntoken = ntokens,
        ninp = 512,
        nhid = 256,
        nlayers = 5,
    )

    model.load_state_dict(
        torch.load(
            model_data_filepath + 'word_language_model_quantize.pth',
            map_location=torch.device('cpu'),
            weights_only=True
            )
        )

    model.eval()
    print(model)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    LSTMModel(
      (drop): Dropout(p=0.5, inplace=False)
      (encoder): Embedding(33278, 512)
      (rnn): LSTM(512, 256, num_layers=5, dropout=0.5)
      (decoder): Linear(in_features=256, out_features=33278, bias=True)
    )




.. GENERATED FROM PYTHON SOURCE LINES 171-174

Now let's generate some text to ensure that the pretrained model is working
properly - similarly to before, we follow
`here <https://github.com/pytorch/examples/blob/master/word_language_model/generate.py>`_

.. GENERATED FROM PYTHON SOURCE LINES 174-199

.. code-block:: default


    input_ = torch.randint(ntokens, (1, 1), dtype=torch.long)
    hidden = model.init_hidden(1)
    temperature = 1.0
    num_words = 1000

    with open(model_data_filepath + 'out.txt', 'w') as outf:
        with torch.no_grad():  # no tracking history
            for i in range(num_words):
                output, hidden = model(input_, hidden)
                word_weights = output.squeeze().div(temperature).exp().cpu()
                word_idx = torch.multinomial(word_weights, 1)[0]
                input_.fill_(word_idx)

                word = corpus.dictionary.idx2word[word_idx]

                outf.write(str(word.encode('utf-8')) + ('\n' if i % 20 == 19 else ' '))

                if i % 100 == 0:
                    print('| Generated {}/{} words'.format(i, 1000))

    with open(model_data_filepath + 'out.txt', 'r') as outf:
        all_output = outf.read()
        print(all_output)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    | Generated 0/1000 words
    | Generated 100/1000 words
    | Generated 200/1000 words
    | Generated 300/1000 words
    | Generated 400/1000 words
    | Generated 500/1000 words
    | Generated 600/1000 words
    | Generated 700/1000 words
    | Generated 800/1000 words
    | Generated 900/1000 words
    b'style' b',' b'and' b'swords' b'might' b'be' b'moved' b'to' b'16' b'mph' b'(' b'10' b'm' b')' b'after' b'being' b'centered' b'in' b'front' b'('
    b'intended' b'being' b'wheat' b')' b',' b'along' b'in' b'the' b'final' b'since' b'which' b'for' b'west' b'14' b'\xe2\x80\x93' b'80' b'%' b'of' b'males' b'in'
    b'which' b',' b'as' b'he' b'is' b'willing' b'to' b'obtain' b'the' b'oil' b'<unk>' b'to' b'have' b'seen' b',' b'they' b'thing' b'wind' b'and' b'distance'
    b'back' b'everything' b'down' b'.' b'Canatara' b'was' b'highly' b'waiting' b'in' b'head' b'iambic' b'Machine' b"'s" b'rebelled' b'that' b'they' b'marksman' b'disagreement' b'going' b'out'
    b'on' b'the' b'first' b'year' b'.' b'In' b'the' b'end' b'on' b'diagnosis' b',' b'Ceres' b'kills' b'the' b'show' b"'s" b'body' b',' b'ending' b'so'
    b'prompting' b'to' b'seized' b'crop' b'Crash' b'.' b'By' b'where' b'he' b'does' b'not' b'control' b'plans' b'the' b'Commander' b'is' b'little' b'much' b'Farmhouse' b'for'
    b'the' b'ground' b'so' b'they' b'went' b'to' b'occasional' b'attention' b'or' b'he' b'fired' b'downgraded' b'possesses' b'.' b'Among' b'that' b'and' b'lost' b'their' b'value'
    b'from' b'Thomas' b'Yamamoto' b'and' b'admits' b'a' b'member' b'of' b'Ceres' b'tells' b'<unk>' b'itself' b'it' b'felt' b'that' b'he' b'had' b'all' b'breaking' b'anyone'
    b'Oh' b'(' b'who' b'reaches' b'sole' b'circles' b'or' b'aggressively' b')' b'design' b'.' b'Naturally' b'sound' b')' b'is' b'always' b'consumed' b'at' b'denotes' b'a'
    b'unanimous' b'small' b'Largs' b'who' b'could' b'continue' b'to' b'keep' b'But' b'the' b'white' b'@-@' b'frequency' b'periodic' b'(' b'<unk>' b')' b'mammals' b'undertake' b'sleeves'
    b',' b'citing' b'such' b'as' b'thought' b'that' b'are' b'hypothesized' b'should' b'have' b'been' b'970' b',' b'especially' b'like' b'a' b'induced' b'@-@' b'egg' b'cycle'
    b'and' b'flock' b'behind' b'them' b'.' b'The' b'ship' b'can' b'do' b'not' b'gain' b'without' b'human' b'crops' b'through' b'any' b'other' b'breeding' b'agencies' b','
    b'and' b'added' b'it' b'was' b'now' b'exhibited' b'.' b'For' b'example' b',' b'all' b'on' b'28' b'%' b'"' b'have' b'wanted' b'at' b'two' b'times'
    b'Library' b'is' b'taken' b',' b'rather' b'of' b'woodland' b'novels' b'ahead' b'have' b'closed' b',' b'leading' b']' b'as' b',' b'Apostles' b',' b'and' b'a'
    b'yield' b'of' b'<unk>' b'based' b'on' b'to' b'not' b'reach' b'people' b'.' b'"' b'In' b'a' b'range' b'of' b'rehabilitation' b'to' b'have' b'made' b'widespread'
    b'with' b'being' b'drained' b',' b'there' b'is' b'also' b'reports' b',' b'when' b'densities' b'have' b'green' b'color' b'young' b'lines' b'.' b'<eos>' b'S.' b'larvae'
    b'so' b'largely' b'reduced' b'any' b'BHU' b',' b'early' b'diseases' b'when' b'men' b'use' b',' b'which' b'vary' b'<unk>' b'-' b'practically' b'athletes' b'such' b'as'
    b'their' b'situations' b'who' b'may' b'serve' b'for' b'AMISOM' b'<unk>' b'.' b'Apart' b'and' b'low' b'were' b'trimmed' b'to' b'have' b'caused' b'giant' b'measurements' b'and'
    b'foraging' b'their' b'own' b'cycle' b'.' b'This' b'forced' b'their' b'designs' b'.' b'TIME' b'.' b'Gangadhara' b'include' b'it' b'as' b'.' b'This' b'subsequently' b'can'
    b'so' b'be' b'porous' b'and' b'the' b'input' b'god' b'which' b'killed' b',' b'because' b'Communication' b',' b'quite' b'minor' b'when' b'they' b'are' b'deeply' b'always'
    b'\xe2\x80\x94' b'romantic' b'as' b'the' b'glands' b'gradually' b'do' b'not' b'reside' b'towards' b'them' b'throughout' b'their' b'times' b'.' b'<eos>' b'Crash' b'galericulata' b'is' b'transmitted'
    b'in' b'a' b'motion' b'between' b'210' b'metres' b'(' b'mostly' b'in' b'which' b'no' b'or' b'one' b'million' b'or' b'away' b')' b',' b'heaven' b'('
    b'RIAA' b')' b',' b'common' b'spine' b'and' b'blue' b'can' b'be' b'distinguished' b'in' b'effects' b'as' b'to' b'continue' b'.' b'This' b'variant' b'is' b'bacteria'
    b'.' b'It' b'is' b'limits' b'by' b'overhaul' b'when' b'leading' b'to' b'the' b'component' b'of' b'human' b'body' b',' b'suitable' b'birds' b',' b'succumbed' b'to'
    b'<unk>' b',' b'gum' b',' b'have' b'to' b'be' b'seen' b'to' b'hold' b'food' b'as' b'they' b'overlap' b'in' b'the' b'deity' b';' b'they' b'have'
    b'people' b'encircle' b'have' b'gone' b'.' b'<eos>' b'Within' b'example' b',' b'infested' b'<unk>' b'may' b'be' b'wrongly' b'held' b'.' b'These' b'sources' b'can' b'have'
    b'been' b'associated' b'in' b'all' b'pathways' b'during' b'the' b'M\xc4\x81ori' b'.' b'However' b',' b'they' b'causes' b'bad' b',' b'drained' b'as' b'outlook' b',' b'or'
    b'neutron' b'immune' b'.' b'Besides' b'this' b',' b'it' b'has' b'no' b'spectral' b'dead' b'features' b',' b'and' b'the' b'spots' b'may' b'be' b'wide' b';'
    b'he' b'produces' b'poor' b'woodland' b'.' b'A' b'population' b'of' b'reality' b'is' b'described' b'by' b'other' b'mature' b'starlings' b',' b'one' b'of' b'number' b'has'
    b'around' b'40' b'@.@' b'2' b'kilometres' b'(' b'3' b'@.@' b'0' b'mi' b')' b'apart' b'on' b'25' b'August' b'1801' b',' b'<unk>' b'preventing' b'often'
    b'when' b'of' b'plant' b'lines' b'.' b'Downs' b',' b'occasionally' b'yet' b'as' b'members' b'of' b'their' b'boxes' b'for' b'first' b'other' b',' b'it' b'has'
    b'purchased' b'no' b'association' b',' b'with' b'few' b'more' b'LED' b'competition' b'known' b'.' b'<eos>' b'<eos>' b'=' b'=' b'=' b'Sovetskaya' b'plumage' b'=' b'='
    b'=' b'<eos>' b'<eos>' b'Following' b'Kodokan' b'I' b',' b'by' b'the' b'hope' b'of' b'his' b'large' b'mouth' b'over' b'three' b'times' b'of' b'snakes' b'they'
    b'were' b'unknown' b',' b'with' b'infantry' b',' b'backgrounds' b',' b'<unk>' b',' b'<unk>' b',' b'f' b',' b'ed' b'and' b'magnum' b'.' b'<eos>' b'Pr'
    b'starlings' b'will' b'be' b'held' b'as' b'edible' b',' b'although' b'before' b'in' b'the' b'other' b',' b'the' b'this' b'type' b',' b'it' b'does' b'not'
    b'"' b'bear' b'Achaea' b'"' b'toward' b'pair' b'.' b'Its' b'technique' b'can' b'be' b'involved' b'early' b'by' b'also' b'.' b'He' b'structures' b'up' b'substrates'
    b'on' b'their' b'dark' b'effects' b'Airfield' b',' b'producing' b'minor' b'nuclei' b'.' b'"' b'Muhammadiyah' b'"' b'is' b'dark' b'brown' b',' b'or' b'together' b'standing'
    b'to' b'Ceres' b'with' b'<unk>' b',' b'thin' b'boiling' b',' b'Labor' b',' b'membrane' b'and' b'grazing' b',' b'Nxf2' b'@-@' b'transcriptional' b'and' b'white' b','
    b'seeds' b',' b'supplies' b',' b'and' b'reddish' b'conditions' b',' b'which' b'may' b'be' b'enjoys' b'by' b'other' b'molecules' b'.' b'Others' b'may' b'have' b'<unk>'
    b'by' b'<unk>' b'cellular' b'<unk>' b',' b'to' b'take' b'even' b'right' b'by' b'near' b'2' b'%' b'bloc' b'matures' b'by' b'shed' b';' b'it' b'is'
    b'electron' b'another' b'a' b'definition' b'about' b'<unk>' b'(' b'monitor' b'two' b'Chasuble' b')' b',' b'\xce\xb2' b'observed' b'of' b'<unk>' b'(' b'which' b'are' b'myeloid'
    b'<unk>' b')' b'.' b'Females' b'bodies' b'appear' b'to' b'be' b'more' b'adequate' b',' b'with' b'a' b'male' b'agricultural' b'images' b'from' b'by' b'so' b'expanding'
    b'and' b',' b'in' b'the' b'manufacture' b'of' b'another' b',' b'or' b'each' b'side' b'of' b'up' b'.' b'rocks' b'males' b'do' b'not' b'increase' b'up'
    b'.' b'There' b'is' b'generally' b'tilt' b'such' b'on' b'rubbing' b'or' b'jackrabbit' b',' b'but' b'potential' b'probably' b'advises' b'access' b'to' b'enzyme' b',' b'<unk>'
    b'or' b'Iran' b'.' b'Even' b'and' b'arise' b'up' b'6' b'to' b'5' b'in' b'(' b'4' b'@.@' b'9' b'cm' b')' b',' b'this' b'male'
    b'mechanism' b'is' b'still' b'spreading' b'in' b'normal' b'regions' b'.' b'This' b'finger' b'may' b'be' b'headed' b'by' b'males' b'to' b'sea' b'matter' b',' b'forming'
    b'or' b'feed' b'on' b'to' b'be' b'trained' b'.' b'Sauber' b'and' b'delicate' b'revisions' b'are' b'also' b'visible' b'.' b'<eos>' b'However' b',' b'there' b'are'
    b'no' b'Measure' b'750' b'\xe2\x80\x93' b'8' b',' b'noting' b',' b'that' b'their' b'Men' b'might' b'be' b'invaded' b'.' b'There' b'are' b'no' b'model' b':'
    b'"' b'<unk>' b',' b'<unk>' b'by' b'flies' b'.' b'"' b'In' b'1934' b',' b'the' b'Kakapo' b'availability' b'of' b'Metrodome' b'in' b'the' b'Hudson' b'crushed'
    b',' b'or' b'their' b'diva' b'above' b'Earth' b'.' b'In' b'and' b'account' b',' b'Tales' b'displays' b',' b'often' b'written' b'to' b'eat' b'severe' b'factors'





.. GENERATED FROM PYTHON SOURCE LINES 200-205

It's no GPT-2, but it looks like the model has started to learn the structure of
language!

We're almost ready to demonstrate dynamic quantization. We just need to define a few more
helper functions:

.. GENERATED FROM PYTHON SOURCE LINES 205-250

.. code-block:: default


    bptt = 25
    criterion = nn.CrossEntropyLoss()
    eval_batch_size = 1

    # create test data set
    def batchify(data, bsz):
        # Work out how cleanly we can divide the dataset into ``bsz`` parts.
        nbatch = data.size(0) // bsz
        # Trim off any extra elements that wouldn't cleanly fit (remainders).
        data = data.narrow(0, 0, nbatch * bsz)
        # Evenly divide the data across the ``bsz`` batches.
        return data.view(bsz, -1).t().contiguous()

    test_data = batchify(corpus.test, eval_batch_size)

    # Evaluation functions
    def get_batch(source, i):
        seq_len = min(bptt, len(source) - 1 - i)
        data = source[i:i+seq_len]
        target = source[i+1:i+1+seq_len].reshape(-1)
        return data, target

    def repackage_hidden(h):
      """Wraps hidden states in new Tensors, to detach them from their history."""

      if isinstance(h, torch.Tensor):
          return h.detach()
      else:
          return tuple(repackage_hidden(v) for v in h)

    def evaluate(model_, data_source):
        # Turn on evaluation mode which disables dropout.
        model_.eval()
        total_loss = 0.
        hidden = model_.init_hidden(eval_batch_size)
        with torch.no_grad():
            for i in range(0, data_source.size(0) - 1, bptt):
                data, targets = get_batch(data_source, i)
                output, hidden = model_(data, hidden)
                hidden = repackage_hidden(hidden)
                output_flat = output.view(-1, ntokens)
                total_loss += len(data) * criterion(output_flat, targets).item()
        return total_loss / (len(data_source) - 1)








.. GENERATED FROM PYTHON SOURCE LINES 251-260

4. Test dynamic quantization
----------------------------

Finally, we can call ``torch.quantization.quantize_dynamic`` on the model!
Specifically,

- We specify that we want the ``nn.LSTM`` and ``nn.Linear`` modules in our
  model to be quantized
- We specify that we want weights to be converted to ``int8`` values

.. GENERATED FROM PYTHON SOURCE LINES 260-268

.. code-block:: default


    import torch.quantization

    quantized_model = torch.quantization.quantize_dynamic(
        model, {nn.LSTM, nn.Linear}, dtype=torch.qint8
    )
    print(quantized_model)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    LSTMModel(
      (drop): Dropout(p=0.5, inplace=False)
      (encoder): Embedding(33278, 512)
      (rnn): DynamicQuantizedLSTM(512, 256, num_layers=5, dropout=0.5)
      (decoder): DynamicQuantizedLinear(in_features=256, out_features=33278, dtype=torch.qint8, qscheme=torch.per_tensor_affine)
    )




.. GENERATED FROM PYTHON SOURCE LINES 269-271

The model looks the same; how has this benefited us? First, we see a
significant reduction in model size:

.. GENERATED FROM PYTHON SOURCE LINES 271-280

.. code-block:: default


    def print_size_of_model(model):
        torch.save(model.state_dict(), "temp.p")
        print('Size (MB):', os.path.getsize("temp.p")/1e6)
        os.remove('temp.p')

    print_size_of_model(model)
    print_size_of_model(quantized_model)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Size (MB): 113.944455
    Size (MB): 79.738939




.. GENERATED FROM PYTHON SOURCE LINES 281-285

Second, we see faster inference time, with no difference in evaluation loss:

Note: we set the number of threads to one for single threaded comparison, since quantized
models run single threaded.

.. GENERATED FROM PYTHON SOURCE LINES 285-297

.. code-block:: default


    torch.set_num_threads(1)

    def time_model_evaluation(model, test_data):
        s = time.time()
        loss = evaluate(model, test_data)
        elapsed = time.time() - s
        print('''loss: {0:.3f}\nelapsed time (seconds): {1:.1f}'''.format(loss, elapsed))

    time_model_evaluation(model, test_data)
    time_model_evaluation(quantized_model, test_data)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    loss: 5.167
    elapsed time (seconds): 198.5
    loss: 5.168
    elapsed time (seconds): 111.0




.. GENERATED FROM PYTHON SOURCE LINES 298-309

Running this locally on a MacBook Pro, without quantization, inference takes about 200 seconds,
and with quantization it takes just about 100 seconds.

Conclusion
----------

Dynamic quantization can be an easy way to reduce model size while only
having a limited effect on accuracy.

Thanks for reading! As always, we welcome any feedback, so please create an issue
`here <https://github.com/pytorch/pytorch/issues>`_ if you have any.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 5 minutes  18.625 seconds)


.. _sphx_glr_download_advanced_dynamic_quantization_tutorial.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example


    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: dynamic_quantization_tutorial.py <dynamic_quantization_tutorial.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: dynamic_quantization_tutorial.ipynb <dynamic_quantization_tutorial.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
